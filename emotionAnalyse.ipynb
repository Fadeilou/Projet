{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import numpy as np\n",
    "import cv2\n",
    "import csv\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('data2.csv','w',newline='') as fichiercsv:\n",
    "    writer=csv.writer(fichiercsv)\n",
    "    writer.writerow(['Nom', 'Prénom', 'E-mail', 'Télephone'])\n",
    "    writer.writerow(['Depond', 'Marcel', 'Marcel@gmail.com', '1020304050'])\n",
    "    writer.writerow(['Alicat', 'Patricia', 'Alicatpa@gmail.com', '1224455660'])\n",
    "    writer.writerow(['Muller', 'Antoni', 'Antoni.muller@gmail.com', '1669988445'])\n",
    "    writer.writerow(['Massont', 'Rodulf', 'Massant.rodulf@gmail.com', '1669988444'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape = (48,48,3)):\n",
    "    \n",
    "    i = tf.keras.layers.Input(input_shape, dtype = tf.uint8)\n",
    "    x = tf.cast(i, tf.float32)\n",
    "    x = tf.keras.applications.vgg16.preprocess_input(x)\n",
    "    \n",
    "    backbone = tf.keras.applications.vgg16.VGG16(\n",
    "                include_top=False, weights='imagenet',\n",
    "                input_tensor=x\n",
    "            )\n",
    "    output_layer = backbone.get_layer(\"block5_conv3\").output    \n",
    "    \n",
    "    \n",
    "    def build_age_branch(input_tensor): \n",
    "        x = tf.keras.layers.Dense(1024, activation = LeakyReLU(alpha=0.3))(input_tensor)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        x = tf.keras.layers.Dense(1, activation = None, name = 'age_output')(x)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "    def build_etchnicity_branch(input_tensor): \n",
    "        x = tf.keras.layers.Dense(500, activation = LeakyReLU(alpha=0.3))(input_tensor)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        x = tf.keras.layers.Dense(5, activation = 'softmax', name = 'ethnicity_output')(x)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "    def build_gender_branch(input_tensor): \n",
    "        x = tf.keras.layers.Dense(500, activation = LeakyReLU(alpha=0.3))(input_tensor)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        x = tf.keras.layers.Dense(1, activation = 'sigmoid', name = 'gender_output')(x)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "    x = tf.keras.layers.Flatten()(output_layer)       \n",
    "    output_age = build_age_branch(x)\n",
    "    output_ethnicity = build_etchnicity_branch(x)\n",
    "    output_gender = build_gender_branch(x)\n",
    "    model = tf.keras.Model(i, [output_age, output_ethnicity, output_gender])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def model_compile():\n",
    "    model = build_model()\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=1e-4), loss = ['mse', 'categorical_crossentropy', 'binary_crossentropy'], loss_weights = [0.001,0.5,0.5], \n",
    "                metrics = {'age_output': 'mean_absolute_error', 'ethnicity_output': 'accuracy', 'gender_output': 'accuracy'})\n",
    "\n",
    "    plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.3, patience=2, verbose = 1\n",
    "    )\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                                        patience=5)  # Early stopping (stops training when validation doesn't improve for {patience} epochs)\n",
    "    save_best = tf.keras.callbacks.ModelCheckpoint('weights.h5', monitor='val_loss', save_best_only=True,\n",
    "                                                mode='min', save_weights_only = True)  # Saves the best version of the model to disk (as measured on the validation data set)\n",
    "    remote_monitor_callback = tf.keras.callbacks.RemoteMonitor(\n",
    "        root='https://dweet.io', path='/dweet/for/multitask',\n",
    "        send_as_json=False, field = 'data'\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "process_this_frame = True\n",
    "\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "names_ethnicity = ['white', 'black', 'asian', 'indian', 'other']\n",
    "name_genders = ['male', 'female']\n",
    "ageList = ['(0-2)', '(3-7)', '(8-15)', '(16-30)', '(30-45)', '(46-60)', '(61-75)', '(76-100)']\n",
    "currentframe = 0\n",
    "data_path = \"public/data6.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(data_path,'w',newline='') as fichiercsv:\n",
    "    writer=csv.writer(fichiercsv)\n",
    "    writer.writerow(['id', 'emotion', 'age', 'genre', 'race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "emotion_model = keras.models.load_model('ml_models/ferNet.h5')\n",
    "model.load_weights(\"ml_models/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def video_analyse(path):\n",
    "    video_capture = cv2.VideoCapture(path)\n",
    "    ret, frame = video_capture.read()\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "    out = cv2.VideoWriter('output.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))\n",
    "    print(\"Processing Video...\")\n",
    "\n",
    "    while video_capture.isOpened():\n",
    "        face_names = []\n",
    "        face_locations = []\n",
    "        face_encodings = []\n",
    "\n",
    "        ret, frame = video_capture.read()\n",
    "        if ret:\n",
    "            face_locations = face_recognition.api.face_locations(frame)\n",
    "            \n",
    "\n",
    "            face_encodings = face_recognition.face_encodings(frame, face_locations)\n",
    "            \n",
    "            \n",
    "            for face_encoding in face_encodings:\n",
    "                \n",
    "                if not known_face_encodings:\n",
    "                    known_face_encodings.append(face_encoding)\n",
    "\n",
    "                    new_name = 1\n",
    "                    known_face_names.append(new_name)\n",
    "\n",
    "                matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "                face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_face_names[best_match_index]\n",
    "                    face_names.append(name)\n",
    "                else:\n",
    "                    known_face_encodings.append(face_encoding)\n",
    "                    new_name = len(known_face_names) + 1\n",
    "                    known_face_names.append(new_name)\n",
    "                    face_names.append(new_name)\n",
    "                \n",
    "\n",
    "            for (top, right, bottom, left), name, face_encoding in zip(face_locations, face_names, face_encodings):\n",
    "                cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "                gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                gray_frame1 = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2RGB)\n",
    "                \n",
    "                roi_frame = gray_frame[top + 50:bottom + 10, left:right]\n",
    "                roi_frame1 = gray_frame1[top + 50:bottom + 10, left:right]\n",
    "\n",
    "                font = cv2.FONT_HERSHEY_DUPLEX\n",
    "\n",
    "                try:\n",
    "                    cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_frame, (48, 48)), -1), 0)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    cropped_img1 = np.expand_dims(cv2.resize(roi_frame1, (48, 48)), 0)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                emotion_prediction = emotion_model.predict(cropped_img)\n",
    "                maxindex = int(np.argmax(emotion_prediction))\n",
    "                emotion = emotion_dict[maxindex]\n",
    "\n",
    "                p = model.predict(cropped_img1)\n",
    "                index = 0\n",
    "\n",
    "                gender_predictions = tf.where(p[2] > 0.5, 1, 0)\n",
    "                race = names_ethnicity[p[1][index].argmax()]\n",
    "                gender = name_genders[gender_predictions[index][0]]\n",
    "                age = p[0][index].astype(np.int)[0]\n",
    "                \n",
    "                if (age >= 0 and age <= 2):\n",
    "                    age_interval = \"(0-2)\"\n",
    "                elif (age>=3 and age <= 7):\n",
    "                    age_interval = \"(3-7)\"\n",
    "                elif (age>=8 and age <= 15):\n",
    "                    age_interval = \"(8-15)\"\n",
    "                elif (age>=16 and age <= 30):\n",
    "                    age_interval = \"(16-30)\"\n",
    "                elif (age>=30 and age <= 45):\n",
    "                    age_interval = \"(30-45)\"\n",
    "                elif (age>=46 and age <= 60):\n",
    "                    age_interval = \"(46-60)\"\n",
    "                elif (age>=61 and age <= 75):\n",
    "                    age_interval = \"(61-75)\"\n",
    "                elif (age>=76 and age <= 100):\n",
    "                    age_interval = \"(76-100)\"\n",
    "                print(emotion)\n",
    "                label = \"{},{},{},{},{}\".format(str(name), emotion, race, gender, age_interval)\n",
    "\n",
    "                with open(data_path,'a',newline='') as fichiercsv:\n",
    "                    writer=csv.writer(fichiercsv)\n",
    "                    writer.writerow([str(name), emotion, age_interval, gender, race])\n",
    "\n",
    "                cv2.putText(frame, label, (left + 6, bottom - 6), font, 1.0, (255, 215, 0), 1)\n",
    "\n",
    "            out.write(frame)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    out.release()\n",
    "    print(\"Done processing video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Video...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-19e07fbb76cb>:72: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  age = p[0][index].astype(np.int)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Angry\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-48dc202d6666>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvideo_analyse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"public/video_test.mp4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-19e07fbb76cb>\u001b[0m in \u001b[0;36mvideo_analyse\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_capture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mface_locations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/face_recognition/api.py\u001b[0m in \u001b[0;36mface_locations\u001b[0;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_trim_css_to_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_rect_to_css\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mface\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_raw_face_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cnn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_trim_css_to_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_rect_to_css\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mface\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_raw_face_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/face_recognition/api.py\u001b[0m in \u001b[0;36m_raw_face_locations\u001b[0;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcnn_face_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "video_analyse(\"public/video_test.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def emotion_proportion(path):\n",
    "    df = pd.read_csv(path)\n",
    "    emotion_proportion = \"\"\n",
    "    total_emotion = df['emotion'].count()\n",
    "    print(total_emotion)\n",
    "\n",
    "    for emotion in emotion_dict:\n",
    "        count = 0\n",
    "        for indice, ligne in df.iterrows():\n",
    "            if ligne['emotion'] == emotion_dict[emotion]:\n",
    "                count = count + 1\n",
    "            proportions = (count * 100) / total_emotion\n",
    "            x = emotion_dict[emotion]+\":\"+str(proportions)+\"-\"\n",
    "        emotion_proportion = emotion_proportion + x\n",
    "\n",
    "    return emotion_proportion\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def age_proportion(path):\n",
    "    df = pd.read_csv(path)\n",
    "    age_proportion = \"\"\n",
    "    total_age = df['age'].count()\n",
    "    print(total_age)\n",
    "\n",
    "    for age in ageList:\n",
    "        count = 0\n",
    "        for indice, ligne in df.iterrows():\n",
    "            if ligne['age'] == age:\n",
    "                count = count + 1\n",
    "            proportions = (count * 100) / total_age\n",
    "            x = gender+\":\"+str(proportions)+\"-\"\n",
    "        age_proportion = age_proportion + x\n",
    "    return age_proportion\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gender_proportion(path):\n",
    "    df = pd.read_csv(path)\n",
    "    gender_proportion = \"\"\n",
    "    total_gender = df['age'].count()\n",
    "    print(total_gender)\n",
    "\n",
    "    for gender in name_genders:\n",
    "        count = 0\n",
    "        for indice, ligne in df.iterrows():\n",
    "            if ligne['genre'] == gender:\n",
    "                count = count + 1\n",
    "            proportions = (count * 100) / total_gender\n",
    "            x = gender+\":\"+str(proportions)+\"-\"\n",
    "        gender_proportion = gender_proportion + x\n",
    "    return gender_proportion\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def race_proportion(path):\n",
    "    df = pd.read_csv(path)\n",
    "    race_proportion = \"\"\n",
    "    total_race = df['age'].count()\n",
    "    print(total_race)\n",
    "\n",
    "    for race in names_ethnicity:\n",
    "        count = 0\n",
    "        for indice, ligne in df.iterrows():\n",
    "            if ligne['race'] == race:\n",
    "                count = count + 1\n",
    "            proportions = (count * 100) / total_race\n",
    "            x = race+\":\"+str(proportions)+\"-\"\n",
    "        race_proportion = race_proportion + x\n",
    "    return race_proportion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def emotion_proportion_according_paramater(path):\n",
    "    df = pd.read_csv(path)\n",
    "    multipe_proportions = \"\"\n",
    "    total_emotion = df['emotion'].count()\n",
    "\n",
    "    for gender in name_genders:   \n",
    "        for race in names_ethnicity:\n",
    "            for age in ageList:\n",
    "                for emotion in emotion_dict:\n",
    "                    count = 0\n",
    "                    for indice, ligne in df.iterrows():\n",
    "                        if ligne['genre'] == gender and ligne['race'] == race and ligne['age'] == age and  ligne['emotion']== emotion_dict[emotion]:\n",
    "                            count = count + 1\n",
    "                    proportion = (count * 100) / total_emotion\n",
    "\n",
    "                    if proportion <= 0:\n",
    "                        continue\n",
    "                    x = str(gender)+\":\"+str(race)+\":\"+str(age)+\":\"+emotion_dict[emotion]+\":\"+str(proportion)+\"-\"\n",
    "                    multipe_proportions = multipe_proportions + x\n",
    "    return multipe_proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(emotion_proportion_according_paramater(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(emotion_proportion(data_path))\n",
    "print(race_proportion(data_path))\n",
    "print(gender_proportion(data_path))\n",
    "print(age_proportion(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "identifiants = df['id']\n",
    "idTab = []\n",
    "for id in identifiants:\n",
    "    if not id in idTab:\n",
    "        idTab.append(id)\n",
    "print(idTab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in idTab:\n",
    "    for j, ligne in df.iterrows():\n",
    "        if ligne['id'] == i:\n",
    "            print(ligne['emotion'])\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "emotion_proportion = {}\n",
    "age_proportion = {}\n",
    "gender_proportion = {}\n",
    "race_proportion = {}\n",
    "total_emotion = df['emotion'].count()\n",
    "print(total_emotion)\n",
    "\n",
    "for emotion in emotion_dict:\n",
    "    count = 0\n",
    "    for indice, ligne in df.iterrows():\n",
    "        if ligne['emotion'] == emotion_dict[emotion]:\n",
    "            count = count + 1\n",
    "        if ligne['race']\n",
    "        emotion_proportion[emotion_dict[emotion]] = (count * 100) / total_emotion\n",
    "\n",
    "print(dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Video...\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "video_capture = cv2.VideoCapture(\"public/video.mp4\")\n",
    "ret, frame = video_capture.read()\n",
    "frame_height, frame_width, _ = frame.shape\n",
    "out = cv2.VideoWriter('output5.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))\n",
    "print(\"Processing Video...\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "while video_capture.isOpened():\n",
    "    idx += 1\n",
    "    face_names = []\n",
    "    face_locations = []\n",
    "    face_encodings = []\n",
    "\n",
    "    ret, frame = video_capture.read()\n",
    "    if ret:\n",
    "        \n",
    "            name = './data/frame' + str(currentframe) + '.jpg'\n",
    "            print('Creating...' + name)\n",
    "\n",
    "            # if process_this_frame:\n",
    "            print(\"-----------------------------------------------------------\")\n",
    "            face_locations = face_recognition.api.face_locations(frame)\n",
    "            print(len(face_locations))\n",
    "\n",
    "            face_encodings = face_recognition.face_encodings(frame, face_locations)\n",
    "            print(len(face_encodings))\n",
    "            print(\"-----------------------------------------------------------\")\n",
    "            # face_names = []\n",
    "            for face_encoding in face_encodings:\n",
    "                print(\"**********************************\")\n",
    "                print(face_encoding)\n",
    "                print(\"**********************************\")\n",
    "\n",
    "                if not known_face_encodings:\n",
    "                    known_face_encodings.append(face_encoding)\n",
    "\n",
    "                    new_name = 1\n",
    "                    known_face_names.append(new_name)\n",
    "\n",
    "                matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "                face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_face_names[best_match_index]\n",
    "                    face_names.append(name)\n",
    "                else:\n",
    "                    known_face_encodings.append(face_encoding)\n",
    "                    new_name = len(known_face_names) + 1\n",
    "                    known_face_names.append(new_name)\n",
    "                    face_names.append(new_name)\n",
    "                print('****name****')\n",
    "                print(face_names)\n",
    "\n",
    "            for (top, right, bottom, left), name, face_encoding in zip(face_locations, face_names, face_encodings):\n",
    "                cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "                # cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n",
    "                gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                gray_frame1 = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2RGB)\n",
    "                \n",
    "                roi_frame = gray_frame[top + 50:bottom + 10, left:right]\n",
    "                roi_frame1 = gray_frame1[top + 50:bottom + 10, left:right]\n",
    "                # gray_frame = cv2.cvtColor(roi_frame, cv2.COLOR_BGR2GRAY)\n",
    "                print(roi_frame1.shape)\n",
    "                font = cv2.FONT_HERSHEY_DUPLEX\n",
    "\n",
    "                cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_frame, (48, 48)), -1), 0)\n",
    "                cropped_img1 = np.expand_dims(cv2.resize(roi_frame1, (48, 48)), 0)\n",
    "                print(cropped_img1)\n",
    "                print(cropped_img1.shape)\n",
    "                # predict the emotions\n",
    "                emotion_prediction = emotion_model.predict(cropped_img)\n",
    "                maxindex = int(np.argmax(emotion_prediction))\n",
    "                emotion = emotion_dict[maxindex]\n",
    "\n",
    "                p = model.predict(cropped_img1)\n",
    "                index = 0\n",
    "\n",
    "                gender_predictions = tf.where(p[2] > 0.5, 1, 0)\n",
    "                race = names_ethnicity[p[1][index].argmax()]\n",
    "                gender = name_genders[gender_predictions[index][0]]\n",
    "                age = p[0][index].astype(np.int)[0]\n",
    "                \n",
    "                if 0 <= age <= 2:\n",
    "                        age_interval = \"(0-2)\"\n",
    "                elif 3 <= age <= 7:\n",
    "                    age_interval = \"(3-7)\"\n",
    "                elif 8 <= age <= 15:\n",
    "                    age_interval = \"(8-15)\"\n",
    "                elif 16 <= age <= 30:\n",
    "                    age_interval = \"(16-30)\"\n",
    "                elif 30 <= age <= 45:\n",
    "                    age_interval = \"(30-45)\"\n",
    "                elif 46 <= age <= 60:\n",
    "                    age_interval = \"(46-60)\"\n",
    "                elif 61 <= age <= 75:\n",
    "                    age_interval = \"(61-75)\"\n",
    "                elif 76 <= age <= 100:\n",
    "                    age_interval = \"(76-100)\"\n",
    "                print(emotion)\n",
    "                label = \"{},{},{},{},{}\".format(str(name), emotion, race, gender, age_interval)\n",
    "\n",
    "                with open('data.csv','a',newline='') as fichiercsv:\n",
    "                    writer=csv.writer(fichiercsv)\n",
    "                    writer.writerow([str(name), emotion, age_interval, gender, race])\n",
    "\n",
    "                cv2.putText(frame, label, (left + 6, bottom - 6), font, 1.0, (255, 215, 0), 1)\n",
    "\n",
    "            out.write(frame)\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    #     break\n",
    "out.release()\n",
    "print(\"Done processing video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while video_capture.isOpened():\n",
    "    idx += 1\n",
    "    face_names = []\n",
    "    face_locations = []\n",
    "    face_encodings = []\n",
    "\n",
    "    ret, frame = video_capture.read()\n",
    "    if ret:\n",
    "        if video_capture.get(cv2.CAP_PROP_POS_FRAMES) == 1:  # Enregistrer 0 seconde image\n",
    "            name = './data/frame' + str(currentframe) + '.jpg'\n",
    "            print('Creating...' + name)\n",
    "\n",
    "            # if process_this_frame:\n",
    "            print(\"-----------------------------------------------------------\")\n",
    "            face_locations = face_recognition.api.face_locations(frame)\n",
    "            print(len(face_locations))\n",
    "\n",
    "            face_encodings = face_recognition.face_encodings(frame, face_locations)\n",
    "            print(len(face_encodings))\n",
    "            print(\"-----------------------------------------------------------\")\n",
    "            # face_names = []\n",
    "            for face_encoding in face_encodings:\n",
    "                print(\"**********************************\")\n",
    "                print(face_encoding)\n",
    "                print(\"**********************************\")\n",
    "\n",
    "                if not known_face_encodings:\n",
    "                    known_face_encodings.append(face_encoding)\n",
    "\n",
    "                    new_name = 1\n",
    "                    known_face_names.append(new_name)\n",
    "\n",
    "                matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "                face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_face_names[best_match_index]\n",
    "                    face_names.append(name)\n",
    "                else:\n",
    "                    known_face_encodings.append(face_encoding)\n",
    "                    new_name = len(known_face_names) + 1\n",
    "                    known_face_names.append(new_name)\n",
    "                    face_names.append(new_name)\n",
    "                print('****name****')\n",
    "                print(face_names)\n",
    "\n",
    "            for (top, right, bottom, left), name, face_encoding in zip(face_locations, face_names, face_encodings):\n",
    "                cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "                # cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n",
    "                gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                gray_frame1 = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2RGB)\n",
    "                \n",
    "                roi_frame = gray_frame[top + 50:bottom + 10, left:right]\n",
    "                roi_frame1 = gray_frame1[top + 50:bottom + 10, left:right]\n",
    "                # gray_frame = cv2.cvtColor(roi_frame, cv2.COLOR_BGR2GRAY)\n",
    "                print(roi_frame1.shape)\n",
    "                font = cv2.FONT_HERSHEY_DUPLEX\n",
    "\n",
    "                cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_frame, (48, 48)), -1), 0)\n",
    "                cropped_img1 = np.expand_dims(cv2.resize(roi_frame1, (48, 48)), 0)\n",
    "                print(cropped_img1)\n",
    "                print(cropped_img1.shape)\n",
    "                # predict the emotions\n",
    "                emotion_prediction = emotion_model.predict(cropped_img)\n",
    "                maxindex = int(np.argmax(emotion_prediction))\n",
    "                emotion = emotion_dict[maxindex]\n",
    "\n",
    "                p = model.predict(cropped_img1)\n",
    "                index = 0\n",
    "\n",
    "                gender_predictions = tf.where(p[2] > 0.5, 1, 0)\n",
    "                race = names_ethnicity[p[1][index].argmax()]\n",
    "                gender = name_genders[gender_predictions[index][0]]\n",
    "                age = p[0][index].astype(np.int)[0]\n",
    "                \n",
    "                if (age >= 0 and age <= 2):\n",
    "                    age_interval = \"(0-2)\"\n",
    "                elif (age>=3 and age <= 7):\n",
    "                    age_interval = \"(3-7)\"\n",
    "                elif (age>=8 and age <= 14):\n",
    "                    age_interval = \"(8-14)\"\n",
    "                elif (age>=15 and age <= 24):\n",
    "                    age_interval = \"(15-24)\"\n",
    "                elif (age>=25 and age <= 37):\n",
    "                    age_interval = \"(25-37)\"\n",
    "                elif (age>=38 and age <= 47):\n",
    "                    age_interval = \"(38-47)\"\n",
    "                elif (age>=48 and age <= 59):\n",
    "                    age_interval = \"(48-59)\"\n",
    "                elif (age>=60 and age <= 100):\n",
    "                    age_interval = \"(60-100)\"\n",
    "                print(emotion)\n",
    "                label = \"{},{},{},{},{}\".format(str(name), emotion, race, gender, age_interval)\n",
    "\n",
    "                with open('data.csv','a',newline='') as fichiercsv:\n",
    "                    writer=csv.writer(fichiercsv)\n",
    "                    writer.writerow([str(name), emotion, age_interval, gender, race])\n",
    "\n",
    "                cv2.putText(frame, label, (left + 6, bottom - 6), font, 1.0, (255, 215, 0), 1)\n",
    "\n",
    "            out.write(frame)\n",
    "\n",
    "        elif idx < video_capture.get(cv2.CAP_PROP_FPS):\n",
    "            continue\n",
    "        else:  # Enregistrer l'image d'une seconde\n",
    "            second = int(video_capture.get(cv2.CAP_PROP_POS_FRAMES) / idx)\n",
    "            name = './data/frame' + str(second) + '.jpg'\n",
    "            print('Creating...' + name)\n",
    "\n",
    "            # if process_this_frame:\n",
    "            print(\"-----------------------------------------------------------\")\n",
    "            face_locations = face_recognition.api.face_locations(frame)\n",
    "            print(len(face_locations))\n",
    "\n",
    "            face_encodings = face_recognition.face_encodings(frame, face_locations)\n",
    "            print(len(face_encodings))\n",
    "            print(\"-----------------------------------------------------------\")\n",
    "            # face_names = []\n",
    "            for face_encoding in face_encodings:\n",
    "                print(\"**********************************\")\n",
    "                print(face_encoding)\n",
    "                print(\"**********************************\")\n",
    "\n",
    "                if not known_face_encodings:\n",
    "                    known_face_encodings.append(face_encoding)\n",
    "\n",
    "                    new_name = 1\n",
    "                    known_face_names.append(new_name)\n",
    "\n",
    "                matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "                face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_face_names[best_match_index]\n",
    "                    face_names.append(name)\n",
    "                else:\n",
    "                    known_face_encodings.append(face_encoding)\n",
    "                    new_name = len(known_face_names) + 1\n",
    "                    known_face_names.append(new_name)\n",
    "                    face_names.append(new_name)\n",
    "                print('****name****')\n",
    "                print(face_names)\n",
    "\n",
    "            for (top, right, bottom, left), name, face_encoding in zip(face_locations, face_names, face_encodings):\n",
    "                cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "                # cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n",
    "                gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                gray_frame1 = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2RGB)\n",
    "                \n",
    "                roi_frame = gray_frame[top + 50:bottom + 10, left:right]\n",
    "                roi_frame1 = gray_frame1[top + 50:bottom + 10, left:right]\n",
    "                # gray_frame = cv2.cvtColor(roi_frame, cv2.COLOR_BGR2GRAY)\n",
    "                print(roi_frame1.shape)\n",
    "                font = cv2.FONT_HERSHEY_DUPLEX\n",
    "\n",
    "                cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_frame, (48, 48)), -1), 0)\n",
    "                cropped_img1 = np.expand_dims(cv2.resize(roi_frame1, (48, 48)), 0)\n",
    "                print(cropped_img1)\n",
    "                print(cropped_img1.shape)\n",
    "                # predict the emotions\n",
    "                emotion_prediction = emotion_model.predict(cropped_img)\n",
    "                maxindex = int(np.argmax(emotion_prediction))\n",
    "                emotion = emotion_dict[maxindex]\n",
    "                print(emotion)\n",
    "\n",
    "                p = model.predict(cropped_img1)\n",
    "                index = 0\n",
    "\n",
    "                gender_predictions = tf.where(p[2] > 0.5, 1, 0)\n",
    "                race = names_ethnicity[p[1][index].argmax()]\n",
    "                gender = name_genders[gender_predictions[index][0]]\n",
    "                age = p[0][index].astype(np.int)[0]\n",
    "                \n",
    "                if (age >= 0 and age <= 2):\n",
    "                    age_interval = \"(0-2)\"\n",
    "                elif (age>=3 and age <= 7):\n",
    "                    age_interval = \"(3-7)\"\n",
    "                elif (age>=8 and age <= 14):\n",
    "                    age_interval = \"(8-14)\"\n",
    "                elif (age>=15 and age <= 24):\n",
    "                    age_interval = \"(15-24)\"\n",
    "                elif (age>=25 and age <= 37):\n",
    "                    age_interval = \"(25-37)\"\n",
    "                elif (age>=38 and age <= 47):\n",
    "                    age_interval = \"(38-47)\"\n",
    "                elif (age>=48 and age <= 59):\n",
    "                    age_interval = \"(48-59)\"\n",
    "                elif (age>=60 and age <= 100):\n",
    "                    age_interval = \"(60-100)\"\n",
    "                print(emotion)\n",
    "                label = \"{},{},{},{},{}\".format(str(name), emotion, race, gender, age_interval)\n",
    "\n",
    "                with open('data.csv','a',newline='') as fichiercsv:\n",
    "                    writer=csv.writer(fichiercsv)\n",
    "                    writer.writerow([str(name), emotion, age_interval, gender, race])\n",
    "\n",
    "                cv2.putText(frame, label, (left + 6, bottom - 6), font, 1.0, (255, 215, 0), 1)\n",
    "            out.write(frame)\n",
    "\n",
    "            # cv2.imshow('Video', frame)\n",
    "            currentframe += 1\n",
    "\n",
    "            idx = 0\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    #     break\n",
    "out.release()\n",
    "print(\"Done processing video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture('video2.mp4')\n",
    "ret, frame = cap.read()\n",
    "frame_height, frame_width, _ = frame.shape\n",
    "out = cv.VideoWriter('output2.avi',cv.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))\n",
    "print(\"Processing Video...\")\n",
    "while cap.isOpened():\n",
    "  ret, frame = cap.read()\n",
    "  if not ret:\n",
    "    out.release()\n",
    "    break\n",
    "  output = age_gender_detector(frame)\n",
    "  out.write(output)\n",
    "out.release()\n",
    "print(\"Done processing video\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2a1d4b6f901dcbecf1dc910fab29dac42dac7feae11371ce9edfe8f2f70cbe6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
